# ...
# Define utility functions for value function iteration
get_u <- function(x, i, theta) {
if (i == 0) {
return(-theta[1] * sqrt(x))
} else {
return(-theta[2])
}
}
get_E_v_next <- function(x, i, V) {
if (i == 0) {
return(0.2 * V[x + 1] + 0.5 * V[x + 2] + 0.3 * V[x + 3])
} else {
return(V[1])  # V(0) when engine is replaced
}
}
get_exp_uv <- function(x, i, V, theta, beta) {
return(exp(get_u(x, i, theta) + beta * get_E_v_next(x, i, V)))
}
get_V_new <- function(X_states, V_old, theta, beta) {
V_new <- numeric(length(X_states))
for (x in X_states) {
numerators <- sapply(0:1, function(i) get_exp_uv(x, i, V_old, theta, beta))
V_new[x + 1] <- log(sum(numerators))
}
return(V_new)
}
get_V <- function(X_states, theta, beta) {
V_initial <- rep(1, length(X_states))
tol <- 1e-3
V_old <- V_initial
V_new <- get_V_new(X_states, V_old, theta, beta)
while (max(abs(V_old - V_new)) > tol) {
V_old <- V_new
V_new <- get_V_new(X_states, V_old, theta, beta)
}
return(V_new)
}
# Define functions for maximum likelihood estimation
get_p <- function(x, i, V, theta, beta) {
numerators <- get_exp_uv(x, i, V, theta, beta)
denominator <- sum(sapply(0:1, function(j) get_exp_uv(x, j, V, theta, beta)))
return(numerators / denominator)
}
get_log_likelihood <- function(X_data, I_data, V, theta, beta) {
log_likelihood <- sum(sapply(1:length(X_data), function(t) {
log(get_p(X_data[t], I_data[t], V, theta, beta))
}))
return(log_likelihood)
}
get_theta <- function(X_data, I_data, beta) {
X_states <- 0:max(X_data)
initial_theta <- c(1, 1)  # Initial guess for theta
V <- get_V(X_states, initial_theta, beta)
optim_result <- optim(
par = initial_theta,
fn = function(theta) -get_log_likelihood(X_data, I_data, V, theta, beta),
method = "L-BFGS-B",
lower = c(0, 0),  # Constrain theta to be positive
upper = c(Inf, Inf)
)
return(optim_result$par)
}
get_theta_from_data <- function(df, beta) {
X_data <- df$engine_mileage
I_data <- df$replace_engine
return(get_theta(X_data, I_data, beta))
}
source("../test/test.R")
# Load necessary libraries
library(Matrix)  # For sparse matrix operations
library(MASS)    # For optimization
library(readr)
# Load necessary libraries
library(maxLik)
# Function to calculate flow utility
get_u <- function(x, i, theta) {
if (i == 0) {
return(-theta[1] * sqrt(x))
} else {
return(-theta[2])
}
}
get_E_v_next <- function(x, i, V) {
if (i == 1) {
# When the engine is replaced, return the value at x=0
return(V[1])
} else {
# When the engine is not replaced, use the transition matrix
# index starts in 1 not in 0 duh R issue
v_index <- x + 1
integration_result <- 0.2*(V[min(v_index, length(V))]) + 0.5 * (V[min(v_index + 1, length(V))]) + 0.3*(V[min(v_index +2, length(V))])
return(integration_result)
}
}
# Function to calculate discounted expected utility
get_exp_uv <- function(x, i, V, theta, beta) {
u_value <- get_u(x, i, theta)
e_v_next <- get_E_v_next(x, i, V)
return(exp(u_value + beta * e_v_next))
}
# Function to calculate the new value function using fixed-point iteration
get_V_new <- function(X_states, V_old, theta, beta) {
num_states <- length(X_states)
V_new <- numeric(num_states)
for (x_index in 1:num_states) {
x <- X_states[x_index]
# Calculate the sum over choices and take the log
# output at x = 0 == 0.01428158
log_sum_exp_uv <- log(get_exp_uv(x, 0, V_old, theta, beta) +
get_exp_uv(x, 1, V_old, theta, beta))
V_new[x_index] <- log_sum_exp_uv
}
return(V_new)
}
# Function for value function iteration
get_V <- function(X_states, theta, beta) {
# Initialize V
V_old <- rep(1, length(X_states))
diff <- Inf
# Iterate until convergence
while (max(abs(diff)) > 1e-5) {
V_new <- get_V_new(X_states, V_old, theta, beta)
diff <- V_new - V_old
V_old <- V_new
}
return(V_new)
}
X_states <- 0:3
theta <- c(0.2, 3)
beta <- 0.95
get_V(X_states , theta , beta)
check_match <- function(theta_true, theta_sol) {
all(abs(theta_true - theta_sol) < .2 * theta_true)
}
# Test value function iteration
test_fp <- function() {
X_states <- 0:3
theta <- c(.2, 3)
beta <- 0.95
V <- get_V(X_states, theta, beta)
correct_V <- c(-4.248785, -4.564213, -4.691457, -4.760538)
pass <- all(abs(V - correct_V) < 1e-3)
cat("Passed value function iteration test:", pass)
}
# Test full model with outer loop MLE and inner loop fixed point
test_full <- function() {
beta = .95
theta_true_1 = c(.2, 3)
theta_true_2 = c(.1, 2.9)
df_1 = read_csv("../data/mechanic_data_1.csv", show_col_types = FALSE)
df_2 = read_csv("../data/mechanic_data_2.csv", show_col_types = FALSE)
theta_sol_1 = get_theta_from_data(df_1, beta)
theta_sol_2 = get_theta_from_data(df_2, beta)
cat("First test:\n")
cat("\tTrue theta_1: ", theta_true_1[1], "\n")
cat("\tSolved theta_1: ", theta_sol_1[1], "\n")
cat("\tTrue theta_2: ", theta_true_1[2], "\n")
cat("\tSolved theta_2: ", theta_sol_1[2], "\n")
pass_1 = check_match(theta_true_1, theta_sol_1)
cat("\tTest 1 Passed: ", pass_1, "\n")
cat("Second test\n")
cat("\tTrue theta_1: ", theta_true_2[1], "\n")
cat("\tSolved theta_1: ", theta_sol_2[1], "\n")
cat("\tTrue theta_2: ", theta_true_2[2], "\n")
cat("\tSolved theta_2: ", theta_sol_2[2], "\n")
pass_2 = check_match(theta_true_2, theta_sol_2)
cat("\tTest 2 passed: ", pass_2, "\n")
}
test_fp()
# Load necessary libraries
library(Matrix)  # For sparse matrix operations
library(MASS)    # For optimization
library(readr)
# Load necessary libraries
library(maxLik)
# Function to calculate flow utility
get_u <- function(x, i, theta) {
if (i == 0) {
return(-theta[1] * sqrt(x))
} else {
return(-theta[2])
}
}
get_E_v_next <- function(x, i, V) {
if (i == 1) {
# When the engine is replaced, return the value at x=0
return(V[1])
} else {
# When the engine is not replaced, use the transition matrix
# index starts in 1 not in 0 duh R issue
v_index <- x + 1
integration_result <- 0.2*(V[min(v_index, length(V))]) + 0.5 * (V[min(v_index + 1, length(V))]) + 0.3*(V[min(v_index +2, length(V))])
return(integration_result)
}
}
# Function to calculate discounted expected utility
get_exp_uv <- function(x, i, V, theta, beta) {
u_value <- get_u(x, i, theta)
e_v_next <- get_E_v_next(x, i, V)
return(exp(u_value + beta * e_v_next))
}
# Function to calculate the new value function using fixed-point iteration
get_V_new <- function(X_states, V_old, theta, beta) {
num_states <- length(X_states)
V_new <- numeric(num_states)
for (x_index in 1:num_states) {
x <- X_states[x_index]
# Calculate the sum over choices and take the log
# output at x = 0 == 0.01428158
log_sum_exp_uv <- log(get_exp_uv(x, 0, V_old, theta, beta) +
get_exp_uv(x, 1, V_old, theta, beta))
V_new[x_index] <- log_sum_exp_uv
}
return(V_new)
}
# Function for value function iteration
get_V <- function(X_states, theta, beta) {
# Initialize V
V_old <- rep(1, length(X_states))
diff <- Inf
# Iterate until convergence
while (max(abs(diff)) > 1e-5) {
V_new <- get_V_new(X_states, V_old, theta, beta)
diff <- V_new - V_old
V_old <- V_new
}
return(V_new)
}
X_states <- 0:3
theta <- c(0.2, 3)
beta <- 0.95
get_V(X_states , theta , beta)
# Function to calculate choice probability
get_p <- function(x, i, V, theta, beta) {
numerator <- exp(get_u(x, i, theta) + beta * get_E_v_next(x, i, V))
denominator <- sum(exp(get_u(x, 0, theta) + beta * get_E_v_next(x, 0, V)) +
exp(get_u(x, 1, theta) + beta * get_E_v_next(x, 1, V)))
return(numerator / denominator)
}
# Function to calculate log-likelihood
get_log_likelihood <- function(X_data, I_data, V, theta, beta) {
log_likelihood <- 0
for (t in seq_along(X_data)) {
x <- X_data[t]
i <- I_data[t]
# Calculate the log choice probability for the observed choice
log_p_i <- log(get_p(x, i, V, theta, beta))
# Accumulate the log likelihood
log_likelihood <- log_likelihood + log_p_i
}
return(log_likelihood)
}
# Function to optimize: negative log-likelihood
neg_log_likelihood <- function(theta, X_data, I_data, beta) {
# Calculate the value function using the current theta
V <- get_V(X_data, theta, beta)
# Calculate the negative log-likelihood
log_likelihood <- get_log_likelihood(X_data, I_data, V, theta, beta)
return(-log_likelihood)
}
# Function to get optimal theta using MLE
get_theta <- function(X_data, I_data, beta) {
# Initial guess for theta
initial_theta <- c(1, 1)  # Adjust based on your model
# Set up the optimization problem
mle_result <- maxLik(neg_log_likelihood, start = initial_theta, X_data = X_data, I_data = I_data, beta = beta)
# Extract the optimal theta
optimal_theta <- coef(mle_result)
return(optimal_theta)
}
# Helper function to calculate theta using different data frames
get_theta_from_data <- function(df, beta) {
X_data <- df$engine_mileage
I_data <- df$replace_engine
return(get_theta(X_data, I_data, beta))
}
# Source the test file
source("../test/test.R")
# Load required library
library(plm)
# Read the data from the CSV file
data <- read.csv("EmplUK.csv")
# Load required library
library(plm)
# Read the data from the CSV file
data <- read.csv("pset3.csv")
library(plm)
setwd("/Users/veronica/Dropbox/PhD/2024_1/EC_708_Econometrics/EC_708_psets/pset3")
# Read the data from the CSV file
data <- read.csv("pset3.csv")
# Convert firm and year to factors
data$firm <- as.factor(data$firm)
data$year <- as.factor(data$year)
# Estimate the model including lagged employment and other explanatory variables
model <- plm(log(emp) ~ lag(log(emp), 1) + lag(log(emp), 2) + wage + capital + output, data = data, index = c("firm", "year"), model = "within")
# Show the summary of the model
summary(model)
# Perform Wald test for coefficients on lagged employment
wald_test(model, c("lag(log(emp), 1)", "lag(log(emp), 2)"))
data$firm <- as.factor(data$firm)
data$year <- as.factor(data$year)
# Function to estimate system GMM model
estimate_system_gmm <- function(data, lags_emp, lags_wage, lags_capital, lags_output, time_periods) {
model <- pgmm(
formula = log(emp) ~ lag(log(emp), lags_emp) + lag(log(wage), lags_wage) +
lag(log(capital), lags_capital) + lag(log(output), lags_output) | lag(log(emp), 2:time_periods),
data = data, effect = "twoways", model = "onestep"
)
return(model)
}
# Define lag structures for variables
emp_lags <- 1:2
wage_lags <- 0:1
capital_lags <- 0:2
output_lags <- 0:2
# Determine the number of time periods
time_periods <- length(unique(data$year))
# Estimate the model using system GMM with one-step estimator
model_one_step <- estimate_system_gmm(data, emp_lags, wage_lags, capital_lags, output_lags, time_periods)
# Load required library
library(plm)
setwd("/Users/veronica/Dropbox/PhD/2024_1/EC_708_Econometrics/EC_708_psets/pset3")
# Read the data from the CSV file
data <- read.csv("pset3.csv")
# Function to estimate system GMM model
estimate_system_gmm <- function(data, lags_emp, lags_wage, lags_capital, lags_output, time_periods, mode) {
model <- pgmm(
formula = log(emp) ~ lag(log(emp), lags_emp) + lag(log(wage), lags_wage) +
lag(log(capital), lags_capital) + lag(log(output), lags_output) | lag(log(emp), 2:time_periods),
data = data, effect = "twoways", model = mode
)
return(model)
}
# Define lag structures for variables
emp_lags <- 1:2
wage_lags <- 0:1
capital_lags <- 0:2
output_lags <- 0:2
# Determine the number of time periods
time_periods <- length(unique(data$year))
# Estimate the model using system GMM with one-step estimator
model_one_step <- estimate_system_gmm(data, emp_lags, wage_lags, capital_lags, output_lags, time_periods,"onestep")
# Load necessary libraries
library(plm)
# Load the dataset
data <- read.csv("/Users/veronica/Dropbox/PhD/2024_1/EC_708_Econometrics/EC_708_psets/pset3/pset3.csv")
# Define the model formula
formula <- log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), 0:1) +
lag(log(capital), 0:2) + lag(log(output), 0:2)
# Estimate using one-step GMM
model_one_step <- pgmm(formula, data = data, effect = "twoways", model = "onestep")
# Load necessary libraries
library(plm)
# Load the dataset
data <- read.csv("/Users/veronica/Dropbox/PhD/2024_1/EC_708_Econometrics/EC_708_psets/pset3/pset3.csv")
# Define the model formula
formula <- log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), 0:1) +
lag(log(capital), 0:2) + lag(log(output), 0:2) | lag(log(emp), 2:ncol(data))
# Estimate using one-step GMM
model_one_step <- pgmm(formula, data = data, effect = "twoways", model = "onestep")
# Estimate using two-step GMM
model_two_step <- pgmm(formula, data = data, effect = "twoways", model = "twostep")
# Print summaries
summary(model_one_step)
summary(model_two_step, robust = FALSE)
# Perform Wald test
car::linearHypothesis(model_two_step, c("lag(log(capital), 0:2)1 = 0",
"lag(log(capital), 0:2)2 = 0"), vcov = vcov)
# Sargan test for overidentifying restrictions
sargan(model_two_step)
setwd("/Users/veronica/Dropbox/PhD/2024_1/EC_708_Econometrics/EC_708_psets/pset3/")
print.xtable(table_one_step, file = "model_one_step.tex", floating = FALSE, include.rownames = FALSE)
library(xtable)
print.xtable(table_one_step, file = "model_one_step.tex", floating = FALSE, include.rownames = FALSE)
table_one_step <- xtable(summary(model_one_step))
table_one_step <- xtable(summary(model_one_step))
install.packages("stargazer")
library(stargazer)
install.packages("stargazer")
# Load necessary libraries
library(plm)
library(xtable)
setwd("/Users/veronica/Dropbox/PhD/2024_1/EC_708_Econometrics/EC_708_psets/pset3/")
# Load the dataset
data <- read.csv("pset3.csv")
# Define the model formula
formula <- log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), 0:1) +
lag(log(capital), 0:2) + lag(log(output), 0:2) | lag(log(emp), 2:ncol(data))
# Estimate using one-step GMM
model_one_step <- pgmm(formula, data = data, effect = "twoways", model = "onestep")
# Estimate using two-step GMM
model_two_step <- pgmm(formula, data = data, effect = "twoways", model = "twostep")
# Print summaries
summary(model_one_step)
summary(model_two_step, robust = FALSE)
# Perform Wald test
car::linearHypothesis(model_two_step, c("lag(log(capital), 0:2)1 = 0",
"lag(log(capital), 0:2)2 = 0"), vcov = vcov)
# Sargan test for overidentifying restrictions
sargan(model_two_step)
# Export summary tables as LaTeX fragments
# Export summary tables as LaTeX fragments
latex <- stargazer(model_one_step, model_two_step, type = "latex",
se = list(sqrt(diag(vcovHC(model_one_step))), sqrt(diag(vcov(model_two_step)))),
object.names = TRUE, model.numbers = FALSE,
single.row = TRUE, align = TRUE, report = "vcs",
title = "Arellano \\& Bond (1991, Table 4) GMM estimates.",
notes = "standard errors are in parentheses.",
notes.append = FALSE, notes.align = "l",
label = "tab:ab",
dep.var.labels = "{$n_{it} = \\log(\\text{emp}_{it})$}",
covariate.labels = c(
"{$n_{it-1}$}", "{$n_{it-2}$}",
"{$w_{it}$}", "{$w_{it-1}$}",
"{$k_{it}$}", "{$k_{it-1}$}", "{$k_{it-2}$}",
"{$y_{it}$}", "{$y_{it-1}$}", "{$y_{it-2}$}"
))
library(stargazer)
# Load necessary libraries
library(plm)
library(xtable)
library(stargazer)
setwd("/Users/veronica/Dropbox/PhD/2024_1/EC_708_Econometrics/EC_708_psets/pset3/")
# Load the dataset
data <- read.csv("pset3.csv")
# Define the model formula
formula <- log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), 0:1) +
lag(log(capital), 0:2) + lag(log(output), 0:2) | lag(log(emp), 2:ncol(data))
# Estimate using one-step GMM
model_one_step <- pgmm(formula, data = data, effect = "twoways", model = "onestep")
# Estimate using two-step GMM
model_two_step <- pgmm(formula, data = data, effect = "twoways", model = "twostep")
# Print summaries
summary(model_one_step)
summary(model_two_step, robust = FALSE)
# Perform Wald test
car::linearHypothesis(model_two_step, c("lag(log(capital), 0:2)1 = 0",
"lag(log(capital), 0:2)2 = 0"), vcov = vcov)
# Sargan test for overidentifying restrictions
sargan(model_two_step)
# Export summary tables as LaTeX fragments
# Export summary tables as LaTeX fragments
latex <- stargazer(model_one_step, model_two_step, type = "latex",
se = list(sqrt(diag(vcovHC(model_one_step))), sqrt(diag(vcov(model_two_step)))),
object.names = TRUE, model.numbers = FALSE,
single.row = TRUE, align = TRUE, report = "vcs",
title = "Arellano \\& Bond (1991, Table 4) GMM estimates.",
notes = "standard errors are in parentheses.",
notes.append = FALSE, notes.align = "l",
label = "tab:ab",
dep.var.labels = "{$n_{it} = \\log(\\text{emp}_{it})$}",
covariate.labels = c(
"{$n_{it-1}$}", "{$n_{it-2}$}",
"{$w_{it}$}", "{$w_{it-1}$}",
"{$k_{it}$}", "{$k_{it-1}$}", "{$k_{it-2}$}",
"{$y_{it}$}", "{$y_{it-1}$}", "{$y_{it-2}$}"
))
latex <- stargazer(model_one_step, model_two_step, type = "latex",
se = list(sqrt(diag(vcovHC(model_one_step))), sqrt(diag(vcov(model_two_step)))),
object.names = TRUE, model.numbers = FALSE,
single.row = TRUE, align = TRUE, report = "vcs",
notes.append = FALSE, notes.align = "l",
label = "tab:ab",
dep.var.labels = "{$n_{it} = \\log(\\text{emp}_{it})$}",
covariate.labels = c(
"{$n_{it-1}$}", "{$n_{it-2}$}",
"{$w_{it}$}", "{$w_{it-1}$}",
"{$k_{it}$}", "{$k_{it-1}$}", "{$k_{it-2}$}",
"{$y_{it}$}", "{$y_{it-1}$}", "{$y_{it-2}$}"
))
summary(model_one_step)
summary(model_two_step, robust = FALSE)
latex <- stargazer::stargazer(
model_one_step, model_two_step, type = "latex",
se = list(sqrt(diag(vcovHC(a1))), sqrt(diag(vcov(a2)))),
object.names = TRUE, model.numbers = FALSE,
single.row = TRUE, align = TRUE, report = "vcs",
title = "Arellano \\& Bond (1991, Table 4) GMM estimates.",
notes = "standard errors are in parentheses.",
notes.append = FALSE, notes.align = "l",
label = "tab:ab",
dep.var.labels = r"{$n_{it} = \log(\text{emp}_{it})$}",
covariate.labels = c(
r"{$n_{it-1}$}", r"{$n_{it-2}$}",
r"{$w_{it}$}", r"{$w_{it-1}$}",
r"{$k_{it}$}", r"{$k_{it-1}$}", r"{$k_{it-2}$}",
r"{$y_{it}$}", r"{$y_{it-1}$}", r"{$y_{it-2}$}"
)
)
latex <- stargazer::stargazer(
model_one_step, model_two_step, type = "latex",
se = list(sqrt(diag(vcovHC(model_one_step))), sqrt(diag(vcov(model_two_step)))),
object.names = TRUE, model.numbers = FALSE,
single.row = TRUE, align = TRUE, report = "vcs",
title = "Arellano \\& Bond (1991, Table 4) GMM estimates.",
notes = "standard errors are in parentheses.",
notes.append = FALSE, notes.align = "l",
label = "tab:ab",
dep.var.labels = r"{$n_{it} = \log(\text{emp}_{it})$}",
covariate.labels = c(
r"{$n_{it-1}$}", r"{$n_{it-2}$}",
r"{$w_{it}$}", r"{$w_{it-1}$}",
r"{$k_{it}$}", r"{$k_{it-1}$}", r"{$k_{it-2}$}",
r"{$y_{it}$}", r"{$y_{it-1}$}", r"{$y_{it-2}$}"
)
)
summary(model_one_step)
table_one_step <- xtable(summary(model_one_step))
model_two_step
summary(model_two_step)
sargan(model_two_step)
